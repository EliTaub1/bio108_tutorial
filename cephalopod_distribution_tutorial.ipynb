{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliTaub1/bio108_tutorial/blob/main/cephalopod_distribution_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3f1VTg29nJy"
      },
      "source": [
        "# Species Distributions Models\n",
        "\n",
        "Among the most fundamental requirements for conserving a species is knowing the locations it inhabits. While obvious, this presents a real obstacle, especially for marine species where collecting systematic observations can be quite difficult.\n",
        "\n",
        "Species distributions are complex and influenced by a range of factors, both biotic and abiotic.\n",
        "Species can survive only within certain bounds of environmental variables, which is known as their Hutchinsonian (or fundamental) niche. Though biotic factors and other influences such as anthropogenic development may reduce the actual distribution of a species to a smaller range (the so-called \"realized niche\"), the Hutchinsonian niche is still valuable as a way to understand the potential for a species to survive within a certain region.\n",
        "\n",
        "Using this idea of an environmentally-constrained distribution, we can create a Species Distribution Model, which uses the environmental conditions in  locations where a species is known to be present to project where the species will or will not occur.\n",
        "\n",
        "## Motivating Question: Cephalopod Distribution\n",
        "\n",
        "Here, we examine the distribution of the squid species *Todaropsis eblanae* in the Canary Current to create a model of where it is distributed. Cephalopods, a class of molluscs which include squid and octopus, have a diverse range of habitats which they inhabit, making them a valuable taxon for understanding the environmental factors underlying their distributions. Considering the diversity of cephalopod life history patterns, it's important to undersand how species are distributed and what role envrionmental variables have in determining their differing distributions.\n",
        "\n",
        "We are using data from [Luna et al (2025)](https://https://doi.org/10.1111/jbi.15112), which analyzed bathymetric preferences of 90 cephalopod species collected from bottom trawl surveys off the northwest coast of Africa between 2004 and 2012. The data can be found at https://datadryad.org/dataset/doi:10.5061/dryad.15dv41p6j. Luna et al specifically sampled some oceanographic variables where they collected these bottom trawls, but since only depth was made public, we will use a more comprehensive set of oceanographic variables from Bio-ORACLE, a publicly avaiable dataset on worldwide oceanographic variables.\n",
        "\n",
        "##Tutorial Objectives\n",
        "In this tutorial, you will learn:\n",
        "*   Background on Species Distribution Models (SDMs)\n",
        "*   Implementing a Random Forest SDM in Python\n",
        "*   Predicting a species' distribution using a SDM\n",
        "*   Evaluating which environmental variables are most important within a SDM\n",
        "\n",
        "This tutorial contains two sections. The first focuses on the data processing of species occurrence and Bio-ORACLE data, while the second focuses on data anlaysis, using a dataframe created in the first (\"cephalopod_locations\") to build and evaluate a species distribution model. Those more interested in the SDM portion of the tutorial are welcome to skip to the data analysis portion.\n",
        "\n",
        "The data analysis portion features 5 live code segments, where a portion of the code has been eliminated so that students can test their comprehension of what is happening. The answers for these live code segments can be found at the Conclusion section of the tutorial. I recommend that students try to solve these sections on their own, as well as running visualizations of dataframes at the beginning and end of data processing code chunks (using .head(), .info(), .describe(), etc.) so that they understand what data manipulations are taking place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l23SXOFjt5QO"
      },
      "source": [
        "## Importing and installing packages\n",
        "First, we install and import necessary packages that we will use in this analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XP2cS2aDBzbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd9b7a6-e52e-4d55-fd7c-ca4c93b41485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting rioxarray\n",
            "  Downloading rioxarray-0.19.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting rasterstats\n",
            "  Downloading rasterstats-0.20.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pyo_oracle\n",
            "  Downloading pyo_oracle-0.0.5-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.1.31)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from rioxarray) (24.2)\n",
            "Requirement already satisfied: xarray>=2024.7.0 in /usr/local/lib/python3.11/dist-packages (from rioxarray) (2025.1.2)\n",
            "Requirement already satisfied: pyproj>=3.3 in /usr/local/lib/python3.11/dist-packages (from rioxarray) (3.7.1)\n",
            "Collecting fiona (from rasterstats)\n",
            "  Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: simplejson in /usr/local/lib/python3.11/dist-packages (from rasterstats) (3.20.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from rasterstats) (2.1.0)\n",
            "Collecting erddapy (from pyo_oracle)\n",
            "  Downloading erddapy-2.2.4-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from xarray>=2024.7.0->rioxarray) (2.2.2)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from erddapy->pyo_oracle) (0.28.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from erddapy->pyo_oracle) (2025.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->erddapy->pyo_oracle) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->erddapy->pyo_oracle) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->erddapy->pyo_oracle) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->erddapy->pyo_oracle) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray>=2024.7.0->rioxarray) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray>=2024.7.0->rioxarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray>=2024.7.0->rioxarray) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->erddapy->pyo_oracle) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->erddapy->pyo_oracle) (4.13.2)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rioxarray-0.19.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rasterstats-0.20.0-py3-none-any.whl (17 kB)\n",
            "Downloading pyo_oracle-0.0.5-py3-none-any.whl (10 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading erddapy-2.2.4-py3-none-any.whl (24 kB)\n",
            "Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cligj, click-plugins, affine, rasterio, fiona, rasterstats, erddapy, rioxarray, pyo_oracle\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 erddapy-2.2.4 fiona-1.10.1 pyo_oracle-0.0.5 rasterio-1.4.3 rasterstats-0.20.0 rioxarray-0.19.0\n"
          ]
        }
      ],
      "source": [
        "#install packages\n",
        "! pip install rasterio rioxarray rasterstats pyo_oracle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CJ4cabGoAozd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21de1cd-b324-4c13-df3c-f7512f278c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config file doesn't exist, creating it.\n",
            "Created configuration at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/config.ini'.\n"
          ]
        }
      ],
      "source": [
        "#import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "#geospatial packages\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "import rasterio.plot\n",
        "import xarray as xr\n",
        "import rioxarray as rio\n",
        "from shapely.geometry import Point\n",
        "import rasterstats\n",
        "#BIO-Oracle data\n",
        "import pyo_oracle\n",
        "#Data analysis\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj7Wj0mvBPD3"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "Before we get into the data analysis, we need to import and format our data. Using the geospatial tools available in Python, we can combine the coordinate data from the cephalopod distribution data with rasterized climate data from BIO-ORACLE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOirDGV7pSvY"
      },
      "source": [
        "### Cephalopod Distribution\n",
        "The data from Luna et al were saved as a Excel (.xlsx) file. For the purposes of this tutorial, they have already been covnerted to a .csv and uploaded to GitHub.\n",
        "\n",
        "We start by importing the data before turning the Longitude and Latitude values into spatial points that can be read as a GeoDataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MobkDyTcBhFz"
      },
      "outputs": [],
      "source": [
        "#Import cephalopod data\n",
        "cephalopod_path = \"https://raw.githubusercontent.com/EliTaub1/bio108_tutorial/refs/heads/main/Supporting_data_-_Cephalopods_distribution_paper.csv\" #path within google drive, modify as needed\n",
        "cephalopod = pd.read_csv(cephalopod_path)\n",
        "cephalopod = cephalopod.dropna()\n",
        "\n",
        "#create GPD with cephalopod locations as spatial points\n",
        "cephalopod_coords = cephalopod[[\"Long L\", \"Latit L\"]]\n",
        "cephalopod_coords = cephalopod_coords.to_numpy()\n",
        "cephalopod_locations = [Point(xy) for xy in cephalopod_coords] #converts coordinate array into spatial points\n",
        "\n",
        "cephalopod_locations = gpd.GeoDataFrame(cephalopod_locations,\n",
        "                                  columns=['geometry'],\n",
        "                                  crs=\"epsg:4326\")\n",
        "#add family and species data for each data point to our GeoDataFrame\n",
        "cephalopod_locations[\"Species\"] = cephalopod[\"Species\"]\n",
        "\n",
        "#remove NA locations\n",
        "cephalopod_locations = cephalopod_locations.loc[(cephalopod_locations.is_empty) == False]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_htctdUGGPG"
      },
      "source": [
        "### Envrionmental Variables\n",
        "Now, we add in our oceanographic data from Bio-ORACLE (https://bio-oracle.org/index.php). Bio-ORACLE has an associated package, pyo_oracle, which makes it easy to acquire Bio-ORACLE datasets and download them directly into our directory!\n",
        "\n",
        "First, we filter layers to get only the ones we desire and then we download these layers in the temporal and geographical range used in the reference study."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmGz5tGFyf1f",
        "outputId": "9fbfb1dc-93df-4a5f-cc75-782339bc8510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset 'chl_baseline_2000_2018_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/chl_baseline_2000_2018_depthmean_2025-04-22-020203.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'dfe_baseline_2000_2018_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/dfe_baseline_2000_2018_depthmean_2025-04-22-020207.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'o2_baseline_2000_2018_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/o2_baseline_2000_2018_depthmean_2025-04-22-020211.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'no3_baseline_2000_2018_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/no3_baseline_2000_2018_depthmean_2025-04-22-020216.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'thetao_baseline_2000_2019_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/thetao_baseline_2000_2019_depthmean_2025-04-22-020220.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'ph_baseline_2000_2018_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/ph_baseline_2000_2018_depthmean_2025-04-22-020225.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'po4_baseline_2000_2018_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/po4_baseline_2000_2018_depthmean_2025-04-22-020229.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'so_baseline_2000_2019_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/so_baseline_2000_2019_depthmean_2025-04-22-020233.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'swd_baseline_2000_2019_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/swd_baseline_2000_2019_depthmean_2025-04-22-020238.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'sws_baseline_2000_2019_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/sws_baseline_2000_2019_depthmean_2025-04-22-020243.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'si_baseline_2000_2018_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/si_baseline_2000_2018_depthmean_2025-04-22-020246.nc'. File size is 31.9 MB.\n",
            "Downloading dataset 'phyc_baseline_2000_2020_depthmean' as nc file to '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data'.\n",
            "Constraints are:\n",
            "{'time>=': '2004-01-01T00:00:00Z', 'time<=': '2012-01-01T12:00:00Z', 'time_step': 1, 'latitude>=': 8, 'latitude<=': 36, 'latitude_step': 1, 'longitude>=': -20, 'longitude<=': 11, 'longitude_step': 1}\n",
            "Download finished at '/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/phyc_baseline_2000_2020_depthmean_2025-04-22-020251.nc'. File size is 31.9 MB.\n"
          ]
        }
      ],
      "source": [
        "# Get full list of layers available\n",
        "bio_oracle_layers = pyo_oracle.list_layers()\n",
        "\n",
        "#reset pyo_oracle data from previous uses and then recreate folder\n",
        "os.system('rm -rf /usr/local/lib/python3.11/dist-packages/pyo_oracle/data/')\n",
        "os.system('mkdir /usr/local/lib/python3.11/dist-packages/pyo_oracle/data/')\n",
        "\n",
        "#Select only layers of variables taken during the baseline period (2000-2018/2019/2020)\n",
        "baselineLayers = bio_oracle_layers[bio_oracle_layers[\"datasetID\"].str.contains(\"baseline\")]\n",
        "#Select only layers of variables taken at the benthic layer (as this is where the bottom trawls were conducted)\n",
        "usedLayers = baselineLayers[baselineLayers[\"datasetID\"].str.contains(\"depthmean\")]\n",
        "\n",
        "# Define constraints on layers to be in desired temporal range and latitude/longitude range. Here, we use the constraints of our reference study.y\n",
        "constraints = {\n",
        "    \"time>=\": \"2004-01-01T00:00:00Z\",\n",
        "    \"time<=\": \"2012-01-01T12:00:00Z\",\n",
        "    \"time_step\": 1,\n",
        "    \"latitude>=\": 8,\n",
        "    \"latitude<=\": 36,\n",
        "    \"latitude_step\": 1,\n",
        "    \"longitude>=\": -20,\n",
        "    \"longitude<=\": 11,\n",
        "    \"longitude_step\": 1\n",
        "}\n",
        "\n",
        "#Download layers using the IDs remaining after we've filtered\n",
        "for title in usedLayers.datasetID:\n",
        "  pyo_oracle.download_layers(title, constraints=constraints, skip_confirmation = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6kU2dkMmOdy"
      },
      "source": [
        "However, we still have one more major step we need to do. Because our data is given in netcdf file format, many of our geospatial packages (e.g. rasterio) cannot effectively work with them. Thus, we convert our files to geoTIFF format in order to make them accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uaZttQ6szAsD"
      },
      "outputs": [],
      "source": [
        "#Reference: https://help.marine.copernicus.eu/en/articles/5029956-how-to-convert-netcdf-to-geotiff\n",
        "#Create save location for files\n",
        "save_folder = '/content/oceandata/'\n",
        "os.system(('mkdir /content/oceandata/'))\n",
        "\n",
        "#List all files in the directory with .nc suffix\n",
        "pyo_directory = \"/usr/local/lib/python3.11/dist-packages/pyo_oracle/data/\"\n",
        "nc_filenames = os.listdir(pyo_directory)\n",
        "nc_filenames = [s for s in nc_filenames if \".nc\" in s]\n",
        "\n",
        "#Rewrite nc as a .tif as save\n",
        "for filename in nc_filenames:\n",
        "  #open file with name suffix\n",
        "  variable_path = pyo_directory + filename\n",
        "  variable = xr.open_dataset(variable_path)\n",
        "  #only extract the data variable with \"mean\" in the name\n",
        "  var_name = [s for s in variable._variables if \"mean\" in s]\n",
        "  var_data = variable[var_name[0]]\n",
        "  #convert var_data into a format where it can be saved as .tif\n",
        "  var_data = var_data.rio.set_spatial_dims(x_dim='longitude', y_dim='latitude')\n",
        "  var_data = var_data.rio.write_crs(\"epsg:4326\", inplace=True)\n",
        "  #save var_data\n",
        "  save_loc = save_folder + var_name[0] + \".tif\"\n",
        "  var_data.rio.to_raster(save_loc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnnCYLFiPyaB"
      },
      "source": [
        "### Combining Data Sources\n",
        "\n",
        "Now, we extract envriomental data for each of the datapoints and add it back to the cephalopod location GeoDataFrame, creating a combined data frame with survey locations and the environmental parameters there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mBuW2KXCQG0i"
      },
      "outputs": [],
      "source": [
        "#find list of filenames of our saved .tif files\n",
        "tif_filenames = os.listdir(save_folder)\n",
        "\n",
        "for filename in tif_filenames:\n",
        "  #open with rasterio\n",
        "  variable_raster = rasterio.open((save_folder + filename))\n",
        "\n",
        "  #extract envrionmental variable data\n",
        "  variable_results = rasterstats.point_query(\n",
        "      cephalopod_locations,\n",
        "      variable_raster.read(1),\n",
        "      nodata = variable_raster.nodata,\n",
        "      affine = variable_raster.transform,\n",
        "      interpolate='nearest'\n",
        "  )\n",
        "  #add envrionmental variable data back to cephalopod data\n",
        "  cephalopod_locations[filename.replace(\".tif\",'')] = variable_results\n",
        "\n",
        "#remove NA values\n",
        "cephalopod_locations = cephalopod_locations.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFohTojfpw34"
      },
      "source": [
        "##Data Analysis\n",
        "\n",
        "Now, we construct a basic model to predict the distribution of *Todaropsis eblanae*.\n",
        "\n",
        "This seems like an immensely complex task, since we sampled only a small range of locations in the Canary Current and yet we are looking to make a prediction for any point within the area. How can we do that?\n",
        "\n",
        "Luckily, this is where machine learning can be quite valuable! Machine learning methods are designed for using training data and extrapolating it to make predictions for data that has not yet been seen. Using a Random Forest model, we can get a sense of where our species will and won't be present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxsiKMsiZUO"
      },
      "source": [
        "###Building Our Training Data\n",
        "For our first step, we need to format our data in a way that our model can understand.\n",
        "\n",
        "Our model is going to be using environmental conditions to predict *Todaropsis eblanae* distribution, meaning that we need to have data on both locations where *Todaropsis eblanae* is present and ones where it was not found. Most studies that create SDMs do not have these sorts of information and must instead use **pseudo-absences**, where artifical absences are selected across the geographic range being studied. Fortunately, because our dataset is from a collection of cephalopod surveys, we have specific locations where *Todaropsis eblanae* were not found that we can use as real absences for our model.\n",
        "\n",
        "To begin with, we need to make sure each location is counted as a single data point, so that no location gets extra emphasis in our model because multiple cephalopods were found there. We do this while marking each location as one where *Todaropsis eblanae* either was or was not found in our surveys.\n",
        "\n",
        "First, we get each location where *Todaropsis eblanae* was found and eliminate duplicates so that each location is used only once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "nzUXPfsyiZUO",
        "outputId": "e03a9821-256d-4eac-b1a3-36a162a64414"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'(' was never closed (<ipython-input-7-0d7eedaba6cc>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0d7eedaba6cc>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    tod_ebl_locations = tod_ebl_locations.drop_duplicates(subset = ###LIVE CODE 1###) #remove duplicate locations (i.e. ones that share geometry)\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
          ]
        }
      ],
      "source": [
        "#Create locations where study species was found\n",
        "tod_ebl_locations = cephalopod_locations[cephalopod_locations[\"Species\"] == \"Todaropsis eblanae\"]\n",
        "tod_ebl_locations = tod_ebl_locations.drop_duplicates(subset = ###LIVE CODE 1###) #remove duplicate locations (i.e. ones that share geometry)\n",
        "tod_ebl_locations[\"tod_ebl\"] = True #input variable for recording tod_ebl presence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create a dataframe of locations with geometry different than the locations where *Todaropsis eblanae* was found (i.e. places where surveys occurred but *Todaropsis eblanae* was absent)."
      ],
      "metadata": {
        "id": "tKGutOdLKyzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create locations where study species was not found\n",
        "other_locations = cephalopod_locations[###LIVE CODE 2###] #select only locations where tod_ebl was not found (i.e. geometry does not match tod_ebl_locations)\n",
        "other_locations = other_locations.drop_duplicates(subset = \"geometry\") #remove duplicate locations\n",
        "other_locations[\"tod_ebl\"] = False #input variable for recording tod_ebl absence"
      ],
      "metadata": {
        "id": "Si8rrSCUIjfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We combine these presence and absence locations into a single dataframe to use as our model input."
      ],
      "metadata": {
        "id": "xcNmQ2svLBFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine dataframes into single entry\n",
        "frame_concat = [tod_ebl_locations, other_locations]\n",
        "location_frame = pd.concat(frame_concat)"
      ],
      "metadata": {
        "id": "kRyl-wEfImG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD1PNcQxtXVj"
      },
      "source": [
        "Now, we need to create a set of training data of our explanatory variables and response variable, as well as a test dataset for evaluating our model (which can't be used in training as we are testing the ability of our model to extrapolate).\n",
        "For our model, we are hoping to use environment to predict *Todaropsis eblanae* presence/absence, so we will use the former as our data and the latter as our target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzuRKmzttgLA"
      },
      "outputs": [],
      "source": [
        "#Create training and test data\n",
        "location_data = location_frame.drop([\"geometry\", \"Species\",\"tod_ebl\"], axis = 1) #data: environmental variables\n",
        "location_target = ###LIVE CODE 3### #target: tod_ebl occurrence\n",
        "#Randomly divide data into training and test sets\n",
        "traindata, testdata, traintarget, testtarget = train_test_split(location_data, location_target, random_state = 47)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej3kHUQsWXtL"
      },
      "source": [
        "### Model Creation and Validation\n",
        "\n",
        "Let's use the Random Forest Classifier model in SciKit Learn to create our model for the SDM. While the details of how the Random Forest Classifier works is outside the scope of this tutorial, it is a versitile model for using predictive quantitative variables to infer which classification a data point will fit into, making it a perfect fit for using environmental data to classify a species as present or absent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5vNNiHwcSOG"
      },
      "outputs": [],
      "source": [
        "#Feed data into RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=5000, random_state=47)\n",
        "model.fit(traindata, traintarget)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEooa2wVY6Eb"
      },
      "source": [
        "We're not out of the woods just yet! First, we need to validate our model and see how accurate it is. Using our test data set, we have our model predict where *Todaropsis eblanae* is or is not present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRv6GVR5Y1J2"
      },
      "outputs": [],
      "source": [
        "#Generate and test predictions\n",
        "pred = model.predict(testdata)\n",
        "\n",
        "#Create and plot confusion matrix\n",
        "mat = confusion_matrix(###LIVE CODE 4###) #see where target data and predictions match\n",
        "\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues')\n",
        "#Print classifircation report of data\n",
        "print(metrics.classification_report(testtarget, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU8CUoLIc1P1"
      },
      "source": [
        "We've run into a major problem! This model is built around predictive accuracy, so because we had so many more absences than presences in our training model,  there's less penalty for being wrong about the less common category. Our model is thus quite conservative and barely classifies any areas as those where our target species will be present. This means that the model had a recall for presences of only 28%, meaning that just 28% of real presences were selected by the model as locations where *Todaropsis eblanae* might be present.\n",
        "\n",
        "We can correct this by instead using a Balanced Random Forest Classifier. This model uses a technique called bootstrapping to, for every training run, select a random subset of the more common classifier data type, creating a balanced sample of presences and absences. In essence, this model will prioritize being correct for each category equally and thus will be more willing to select a location as a presence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTwwFCoDYeGx"
      },
      "outputs": [],
      "source": [
        "#Feed data into BalancedRandomForestClassifier\n",
        "balanced_model = BalancedRandomForestClassifier(n_estimators=5000, random_state=47)\n",
        "balanced_model.fit(traindata, traintarget)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89jsQhWpSpvn"
      },
      "source": [
        "Let's validate this new model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXuNVqfFSkxl"
      },
      "outputs": [],
      "source": [
        "#Generate and test predictions\n",
        "pred = balanced_model.predict(testdata)\n",
        "\n",
        "#Create and plot confusion matrix\n",
        "balanced_mat = confusion_matrix(testtarget, pred) #see where target data and predictions match\n",
        "\n",
        "sns.heatmap(balanced_mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues')\n",
        "#Print classifircation report of data\n",
        "print(metrics.classification_report(testtarget, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HRiVbLqe-go"
      },
      "source": [
        "While our recall is much improved to 62%, our new model is still very flawed! Our new model had a precision for presences of only 30%, meaning that predicted presences only had a 30% chance of actually being present.\n",
        "\n",
        "We can begin to see some of the tradeoffs that are present in Species Distribution Modelling. It's difficult to get more data, and even if perhaps the underlying model can be improved, there's a limit to predictive capacity. How do we decide what model is best for our purposes?\n",
        "\n",
        "Let's consider our data being used here. We are certain that all locations marked as present contain *Todaropsis eblanae*, but even though our absences are better than pseudo-absences, we still can't be certain *Todaropsis eblanae* is not in the area (just that we Luna et al find any in the bottom trawl). Thus, it seems more likely that our data contains false negatives than false positives, so we will select the model with better recall for future analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-lnrFpciZUP"
      },
      "source": [
        "### Predicting *Todaropsis eblanae* Distribution\n",
        "\n",
        "Now that we've created this model, we make predictions on *Todaropsis eblanae* presence/absence for all points within the study area.\n",
        "\n",
        "To do this, we need to start by formatting our environmental data in a way that the model can process, converting it from a raster to a dataframe. Note that our data is coming from 12 rasters that are formatted as 561 x 621 arrays, and we convert it into a dataframe where each cell has its own row, meaning that we are creating a 12 x (561*621 = 348381) dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0s3qKYwiZUP"
      },
      "outputs": [],
      "source": [
        "#access filenames of environmental variables\n",
        "tif_filenames = os.listdir(save_folder)\n",
        "\n",
        "#create a placeholder array of zeroes to hold the data from the environmental variables\n",
        "holderArray = np.zeros((12, 561, 621))\n",
        "i = 0\n",
        "\n",
        "#create array with all data from the environmental variables\n",
        "for filename in tif_filenames:\n",
        "  #open with rasterio\n",
        "  variable_raster = rasterio.open((save_folder + filename))\n",
        "  #add data from raster to the holderArray\n",
        "  holderArray[[i]] = variable_raster.read(1)\n",
        "  i = i + 1\n",
        "\n",
        "#reformat the data to have 348381 (561*621) rows with each of the 12 envrionmental variables, so it can be fed into our model\n",
        "newHolder = pd.DataFrame(holderArray.reshape(12, 348381).transpose(1,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the predict feature in our Balanced Random Forest Classifier, we can generate predictions for each of the 348381 locations."
      ],
      "metadata": {
        "id": "xza71k_6JsPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use our random forest model on the new data\n",
        "holderpred = balanced_model.predict(newHolder)\n",
        "\n",
        "#reformat the new data to be plottable by reshaping to be a grid and coercing to be a float\n",
        "holderpred = holderpred.reshape(###LIVE CODE 5###).astype(float) #reshape to the original raster dimensions, change data type to float\n",
        "#convert holderpred values corresponding to no data (land areas) to be null\n",
        "holderpred[holderArray[1] == -9999.9] = None"
      ],
      "metadata": {
        "id": "2nd-ISGMJlBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK7Lb-c-dmO7"
      },
      "source": [
        "Now, let's plot our SDM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIN-s_kTxwZn"
      },
      "outputs": [],
      "source": [
        "# Convert the predictions (holderpred) to a DataArray, matching the spatial structure of the original raster\n",
        "predictions_xr = rio.open_rasterio(save_folder + tif_filenames[0]).copy(deep=True)\n",
        "predictions_xr.values[0] = holderpred\n",
        "predictions_xr.attrs[\"ioos_category\"] = \"Presence/Absence\"\n",
        "predictions_xr.attrs[\"long_name\"] = \"SDM Predictions\"\n",
        "predictions_xr.attrs[\"units\"] = \"Presence/Absence\"\n",
        "\n",
        "predictions_xr[0].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRuF6TFLeCS8"
      },
      "source": [
        "We can now see a predicted distribution of *Todaropsis eblanae* (the areas in yellow)! Even in areas where we have not surveyed, our model predicts habitat suitability or lack thereof, which is quite critical considering that it would take immense effort to survey in every 0.05 degree of latitude and longitude on the map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTOEA-RmIor"
      },
      "source": [
        "### Evaluating Environmental Variable Importance\n",
        "\n",
        "But which variables have the greatest importance in limiting *Todaropsis eblanae* distribution? Using the feature_importances feature for our Balanced Random Forest Classifier, we can find out which make the greatest contrabutions to how the classifier makes its decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJxZR50AdZJK"
      },
      "outputs": [],
      "source": [
        "#find importance values\n",
        "importance = balanced_model.feature_importances_\n",
        "\n",
        "#determine corresponding labels\n",
        "labels = np.array(location_data.columns)\n",
        "#format labels so that they no longer include mean!\n",
        "for i in range(12): labels[i] = labels[i].split(\"_\")[0]\n",
        "#convert to series with labels\n",
        "labeled_importance = pd.Series(importance, labels)\n",
        "#sort values from greatest to least\n",
        "labeled_importance = labeled_importance.sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let's plot these importances!"
      ],
      "metadata": {
        "id": "pi4N3l5oKddf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot labeled importance\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "#use bar plot\n",
        "ax.bar(labeled_importance.index, labeled_importance)\n",
        "\n",
        "#Set plot labels\n",
        "ax.set_ylabel('Importance')\n",
        "ax.set_xlabel('Variable')\n",
        "ax.set_title('Relative model importance of climatic variables')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nZe0mnkDKbvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsBvKK7IhVEL"
      },
      "source": [
        "The three most important variables are salinity (so), dissolved oxygen (o2), and pH.\n",
        "\n",
        "However, it is worth noting this could be in part due to spatial autocorrelation, as Luna et al noted that salinity and dissolved oxygen had significant relationships with latitude in the Canary Current and thus this observed relationship may be driven by some confounding variable associated with latitude of location rather than salinity and oxygen themselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmlGT5HjmzqW"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "In this tutorial, we've explored the use of Python to build Species Distribution Models.\n",
        "\n",
        "Using this technique, we were able to predict the distribution of a marine species and explore which environmental variables were most important in determining its distribution. This contributes valuable information for understanding its ecology, making conservation efforts possible for this species.\n",
        "\n",
        "Through participating in this tutorial, you have now gained the skills to create, implement, and evaluate a Random Forest SDM of your own in Python.\n",
        "\n",
        "*Live Code Solutions:*\n",
        "```\n",
        "1. \"geometry\"\n",
        "2. ~ cephalopod_locations[\"geometry\"].isin(tod_ebl_locations.geometry)\n",
        "3. location_frame.tod_ebl\n",
        "4. testtarget, pred\n",
        "5. 561, 621\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM97ZVyzJ+uipGkMaRQQ0Cv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}